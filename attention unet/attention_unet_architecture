digraph {
	graph [size="112.35,112.35"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1992601245792 [label="
 (1, 1, 128, 128)" fillcolor=darkolivegreen1]
	1995905452784 [label=SigmoidBackward0]
	1995936289824 -> 1995905452784
	1995936289824 [label=ConvolutionBackward0]
	1995937063664 -> 1995936289824
	1995937063664 [label=ReluBackward0]
	1995936208336 -> 1995937063664
	1995936208336 [label=NativeBatchNormBackward0]
	1995909109632 -> 1995936208336
	1995909109632 [label=ConvolutionBackward0]
	1995909286928 -> 1995909109632
	1995909286928 [label=ReluBackward0]
	1995904958864 -> 1995909286928
	1995904958864 [label=NativeBatchNormBackward0]
	1995904958576 -> 1995904958864
	1995904958576 [label=ConvolutionBackward0]
	1995904960016 -> 1995904958576
	1995904960016 [label=CatBackward0]
	1995904964864 -> 1995904960016
	1995904964864 [label=MulBackward0]
	1995909375376 -> 1995904964864
	1995909375376 [label=ReluBackward0]
	1995904967312 -> 1995909375376
	1995904967312 [label=NativeBatchNormBackward0]
	1995904961840 -> 1995904967312
	1995904961840 [label=ConvolutionBackward0]
	1995904973072 -> 1995904961840
	1995904973072 [label=ReluBackward0]
	1995904968032 -> 1995904973072
	1995904968032 [label=NativeBatchNormBackward0]
	1995904968560 -> 1995904968032
	1995904968560 [label=ConvolutionBackward0]
	1995904968464 -> 1995904968560
	1991123226288 [label="conv1.conv1.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	1991123226288 -> 1995904968464
	1995904968464 [label=AccumulateGrad]
	1995904968944 -> 1995904968560
	1995914570448 [label="conv1.conv1.bias
 (64)" fillcolor=lightblue]
	1995914570448 -> 1995904968944
	1995904968944 [label=AccumulateGrad]
	1995904969088 -> 1995904968032
	1995905099056 [label="conv1.bn1.weight
 (64)" fillcolor=lightblue]
	1995905099056 -> 1995904969088
	1995904969088 [label=AccumulateGrad]
	1995904964624 -> 1995904968032
	1995905099136 [label="conv1.bn1.bias
 (64)" fillcolor=lightblue]
	1995905099136 -> 1995904964624
	1995904964624 [label=AccumulateGrad]
	1995904968080 -> 1995904961840
	1995905099696 [label="conv1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1995905099696 -> 1995904968080
	1995904968080 [label=AccumulateGrad]
	1995904963040 -> 1995904961840
	1995905099776 [label="conv1.conv2.bias
 (64)" fillcolor=lightblue]
	1995905099776 -> 1995904963040
	1995904963040 [label=AccumulateGrad]
	1995904973552 -> 1995904967312
	1995905099856 [label="conv1.bn2.weight
 (64)" fillcolor=lightblue]
	1995905099856 -> 1995904973552
	1995904973552 [label=AccumulateGrad]
	1995904964336 -> 1995904967312
	1995905099936 [label="conv1.bn2.bias
 (64)" fillcolor=lightblue]
	1995905099936 -> 1995904964336
	1995904964336 [label=AccumulateGrad]
	1995904964672 -> 1995904964864
	1995904964672 [label=SigmoidBackward0]
	1995936204784 -> 1995904964672
	1995936204784 [label=NativeBatchNormBackward0]
	1995904964816 -> 1995936204784
	1995904964816 [label=ConvolutionBackward0]
	1995904968512 -> 1995904964816
	1995904968512 [label=ReluBackward0]
	1995904969856 -> 1995904968512
	1995904969856 [label=AddBackward0]
	1995937004944 -> 1995904969856
	1995937004944 [label=NativeBatchNormBackward0]
	1995904968224 -> 1995937004944
	1995904968224 [label=ConvolutionBackward0]
	1995904964912 -> 1995904968224
	1995904964912 [label=ConvolutionBackward0]
	1995904967648 -> 1995904964912
	1995904967648 [label=ReluBackward0]
	1995904967696 -> 1995904967648
	1995904967696 [label=NativeBatchNormBackward0]
	1995904966496 -> 1995904967696
	1995904966496 [label=ConvolutionBackward0]
	1995904970768 -> 1995904966496
	1995904970768 [label=ReluBackward0]
	1995904970480 -> 1995904970768
	1995904970480 [label=NativeBatchNormBackward0]
	1995904972256 -> 1995904970480
	1995904972256 [label=ConvolutionBackward0]
	1995904970576 -> 1995904972256
	1995904970576 [label=CatBackward0]
	1995904970240 -> 1995904970576
	1995904970240 [label=MulBackward0]
	1995904971584 -> 1995904970240
	1995904971584 [label=ReluBackward0]
	1995904970000 -> 1995904971584
	1995904970000 [label=NativeBatchNormBackward0]
	1995904969904 -> 1995904970000
	1995904969904 [label=ConvolutionBackward0]
	1995904973456 -> 1995904969904
	1995904973456 [label=ReluBackward0]
	1995904973264 -> 1995904973456
	1995904973264 [label=NativeBatchNormBackward0]
	1995904972928 -> 1995904973264
	1995904972928 [label=ConvolutionBackward0]
	1995904972976 -> 1995904972928
	1995904972976 [label=MaxPool2DWithIndicesBackward0]
	1995909375376 -> 1995904972976
	1995904973024 -> 1995904972928
	1995905100416 [label="conv2.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1995905100416 -> 1995904973024
	1995904973024 [label=AccumulateGrad]
	1995904974704 -> 1995904972928
	1995905100496 [label="conv2.conv1.bias
 (128)" fillcolor=lightblue]
	1995905100496 -> 1995904974704
	1995904974704 [label=AccumulateGrad]
	1995904974272 -> 1995904973264
	1995905100576 [label="conv2.bn1.weight
 (128)" fillcolor=lightblue]
	1995905100576 -> 1995904974272
	1995904974272 [label=AccumulateGrad]
	1995904973600 -> 1995904973264
	1995905100656 [label="conv2.bn1.bias
 (128)" fillcolor=lightblue]
	1995905100656 -> 1995904973600
	1995904973600 [label=AccumulateGrad]
	1995904973408 -> 1995904969904
	1995905101136 [label="conv2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1995905101136 -> 1995904973408
	1995904973408 [label=AccumulateGrad]
	1995904973888 -> 1995904969904
	1995905101216 [label="conv2.conv2.bias
 (128)" fillcolor=lightblue]
	1995905101216 -> 1995904973888
	1995904973888 [label=AccumulateGrad]
	1995904971968 -> 1995904970000
	1995905101296 [label="conv2.bn2.weight
 (128)" fillcolor=lightblue]
	1995905101296 -> 1995904971968
	1995904971968 [label=AccumulateGrad]
	1995904971248 -> 1995904970000
	1995905101376 [label="conv2.bn2.bias
 (128)" fillcolor=lightblue]
	1995905101376 -> 1995904971248
	1995904971248 [label=AccumulateGrad]
	1995904971392 -> 1995904970240
	1995904971392 [label=SigmoidBackward0]
	1995904973360 -> 1995904971392
	1995904973360 [label=NativeBatchNormBackward0]
	1995904973216 -> 1995904973360
	1995904973216 [label=ConvolutionBackward0]
	1995904974464 -> 1995904973216
	1995904974464 [label=ReluBackward0]
	1995904972352 -> 1995904974464
	1995904972352 [label=AddBackward0]
	1995904974080 -> 1995904972352
	1995904974080 [label=NativeBatchNormBackward0]
	1995904972448 -> 1995904974080
	1995904972448 [label=ConvolutionBackward0]
	1995904970288 -> 1995904972448
	1995904970288 [label=ConvolutionBackward0]
	1995904973648 -> 1995904970288
	1995904973648 [label=ReluBackward0]
	1995904968992 -> 1995904973648
	1995904968992 [label=NativeBatchNormBackward0]
	1995904967120 -> 1995904968992
	1995904967120 [label=ConvolutionBackward0]
	1995904970672 -> 1995904967120
	1995904970672 [label=ReluBackward0]
	1995904970096 -> 1995904970672
	1995904970096 [label=NativeBatchNormBackward0]
	1995904967792 -> 1995904970096
	1995904967792 [label=ConvolutionBackward0]
	1995904967984 -> 1995904967792
	1995904967984 [label=CatBackward0]
	1995904967552 -> 1995904967984
	1995904967552 [label=MulBackward0]
	1992578027856 -> 1995904967552
	1992578027856 [label=ReluBackward0]
	1992525308368 -> 1992578027856
	1992525308368 [label=NativeBatchNormBackward0]
	1991824598416 -> 1992525308368
	1991824598416 [label=ConvolutionBackward0]
	1992598024352 -> 1991824598416
	1992598024352 [label=ReluBackward0]
	1992555099424 -> 1992598024352
	1992555099424 [label=NativeBatchNormBackward0]
	1992595106032 -> 1992555099424
	1992595106032 [label=ConvolutionBackward0]
	1995904264912 -> 1992595106032
	1995904264912 [label=MaxPool2DWithIndicesBackward0]
	1995904971584 -> 1995904264912
	1995904266832 -> 1992595106032
	1995905101856 [label="conv3.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1995905101856 -> 1995904266832
	1995904266832 [label=AccumulateGrad]
	1995904264720 -> 1992595106032
	1995905101936 [label="conv3.conv1.bias
 (256)" fillcolor=lightblue]
	1995905101936 -> 1995904264720
	1995904264720 [label=AccumulateGrad]
	1995904265248 -> 1992555099424
	1995905102016 [label="conv3.bn1.weight
 (256)" fillcolor=lightblue]
	1995905102016 -> 1995904265248
	1995904265248 [label=AccumulateGrad]
	1995904262848 -> 1992555099424
	1995905102096 [label="conv3.bn1.bias
 (256)" fillcolor=lightblue]
	1995905102096 -> 1995904262848
	1995904262848 [label=AccumulateGrad]
	1992598018304 -> 1991824598416
	1995905102576 [label="conv3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1995905102576 -> 1992598018304
	1992598018304 [label=AccumulateGrad]
	1992598013456 -> 1991824598416
	1995905102656 [label="conv3.conv2.bias
 (256)" fillcolor=lightblue]
	1995905102656 -> 1992598013456
	1992598013456 [label=AccumulateGrad]
	1991824598512 -> 1992525308368
	1995905102736 [label="conv3.bn2.weight
 (256)" fillcolor=lightblue]
	1995905102736 -> 1991824598512
	1991824598512 [label=AccumulateGrad]
	1992598018064 -> 1992525308368
	1995905102816 [label="conv3.bn2.bias
 (256)" fillcolor=lightblue]
	1995905102816 -> 1992598018064
	1992598018064 [label=AccumulateGrad]
	1992578025168 -> 1995904967552
	1992578025168 [label=SigmoidBackward0]
	1992598012208 -> 1992578025168
	1992598012208 [label=NativeBatchNormBackward0]
	1992525308128 -> 1992598012208
	1992525308128 [label=ConvolutionBackward0]
	1995904254832 -> 1992525308128
	1995904254832 [label=ReluBackward0]
	1995904255792 -> 1995904254832
	1995904255792 [label=AddBackward0]
	1995904256032 -> 1995904255792
	1995904256032 [label=NativeBatchNormBackward0]
	1995904266256 -> 1995904256032
	1995904266256 [label=ConvolutionBackward0]
	1995904967024 -> 1995904266256
	1995904967024 [label=ConvolutionBackward0]
	1995904270288 -> 1995904967024
	1995904270288 [label=ReluBackward0]
	1995904267168 -> 1995904270288
	1995904267168 [label=NativeBatchNormBackward0]
	1995904269136 -> 1995904267168
	1995904269136 [label=ConvolutionBackward0]
	1995904266064 -> 1995904269136
	1995904266064 [label=ReluBackward0]
	1995904261360 -> 1995904266064
	1995904261360 [label=NativeBatchNormBackward0]
	1995904261120 -> 1995904261360
	1995904261120 [label=ConvolutionBackward0]
	1995904257568 -> 1995904261120
	1995904257568 [label=CatBackward0]
	1995904256848 -> 1995904257568
	1995904256848 [label=MulBackward0]
	1995904254784 -> 1995904256848
	1995904254784 [label=ReluBackward0]
	1995904257616 -> 1995904254784
	1995904257616 [label=NativeBatchNormBackward0]
	1995904258432 -> 1995904257616
	1995904258432 [label=ConvolutionBackward0]
	1995904260304 -> 1995904258432
	1995904260304 [label=ReluBackward0]
	1995904254016 -> 1995904260304
	1995904254016 [label=NativeBatchNormBackward0]
	1995904262080 -> 1995904254016
	1995904262080 [label=ConvolutionBackward0]
	1995904265440 -> 1995904262080
	1995904265440 [label=MaxPool2DWithIndicesBackward0]
	1992578027856 -> 1995904265440
	1995904257856 -> 1995904262080
	1995905103296 [label="conv4.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1995905103296 -> 1995904257856
	1995904257856 [label=AccumulateGrad]
	1995904268032 -> 1995904262080
	1995905103376 [label="conv4.conv1.bias
 (512)" fillcolor=lightblue]
	1995905103376 -> 1995904268032
	1995904268032 [label=AccumulateGrad]
	1995904264528 -> 1995904254016
	1995905103456 [label="conv4.bn1.weight
 (512)" fillcolor=lightblue]
	1995905103456 -> 1995904264528
	1995904264528 [label=AccumulateGrad]
	1995904259440 -> 1995904254016
	1995905103536 [label="conv4.bn1.bias
 (512)" fillcolor=lightblue]
	1995905103536 -> 1995904259440
	1995904259440 [label=AccumulateGrad]
	1995904256128 -> 1995904258432
	1995905104016 [label="conv4.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1995905104016 -> 1995904256128
	1995904256128 [label=AccumulateGrad]
	1995904256224 -> 1995904258432
	1995905104096 [label="conv4.conv2.bias
 (512)" fillcolor=lightblue]
	1995905104096 -> 1995904256224
	1995904256224 [label=AccumulateGrad]
	1995904258672 -> 1995904257616
	1995905104176 [label="conv4.bn2.weight
 (512)" fillcolor=lightblue]
	1995905104176 -> 1995904258672
	1995904258672 [label=AccumulateGrad]
	1995904268368 -> 1995904257616
	1995905104256 [label="conv4.bn2.bias
 (512)" fillcolor=lightblue]
	1995905104256 -> 1995904268368
	1995904268368 [label=AccumulateGrad]
	1995904254880 -> 1995904256848
	1995904254880 [label=SigmoidBackward0]
	1995904258528 -> 1995904254880
	1995904258528 [label=NativeBatchNormBackward0]
	1995904260256 -> 1995904258528
	1995904260256 [label=ConvolutionBackward0]
	1995904265680 -> 1995904260256
	1995904265680 [label=ReluBackward0]
	1995904265488 -> 1995904265680
	1995904265488 [label=AddBackward0]
	1995904266592 -> 1995904265488
	1995904266592 [label=NativeBatchNormBackward0]
	1995904266880 -> 1995904266592
	1995904266880 [label=ConvolutionBackward0]
	1995904257136 -> 1995904266880
	1995904257136 [label=ConvolutionBackward0]
	1995904264000 -> 1995904257136
	1995904264000 [label=ReluBackward0]
	1995904264576 -> 1995904264000
	1995904264576 [label=NativeBatchNormBackward0]
	1995904267696 -> 1995904264576
	1995904267696 [label=ConvolutionBackward0]
	1995904263136 -> 1995904267696
	1995904263136 [label=ReluBackward0]
	1995904263232 -> 1995904263136
	1995904263232 [label=NativeBatchNormBackward0]
	1995904264768 -> 1995904263232
	1995904264768 [label=ConvolutionBackward0]
	1995904261984 -> 1995904264768
	1995904261984 [label=MaxPool2DWithIndicesBackward0]
	1995904254784 -> 1995904261984
	1995904264048 -> 1995904264768
	1995905104736 [label="conv5.conv1.weight
 (1024, 512, 3, 3)" fillcolor=lightblue]
	1995905104736 -> 1995904264048
	1995904264048 [label=AccumulateGrad]
	1995904266016 -> 1995904264768
	1995905104816 [label="conv5.conv1.bias
 (1024)" fillcolor=lightblue]
	1995905104816 -> 1995904266016
	1995904266016 [label=AccumulateGrad]
	1995904262416 -> 1995904263232
	1995905104896 [label="conv5.bn1.weight
 (1024)" fillcolor=lightblue]
	1995905104896 -> 1995904262416
	1995904262416 [label=AccumulateGrad]
	1995904266112 -> 1995904263232
	1995905104976 [label="conv5.bn1.bias
 (1024)" fillcolor=lightblue]
	1995905104976 -> 1995904266112
	1995904266112 [label=AccumulateGrad]
	1995904260736 -> 1995904267696
	1995905105456 [label="conv5.conv2.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	1995905105456 -> 1995904260736
	1995904260736 [label=AccumulateGrad]
	1995904264192 -> 1995904267696
	1995905105536 [label="conv5.conv2.bias
 (1024)" fillcolor=lightblue]
	1995905105536 -> 1995904264192
	1995904264192 [label=AccumulateGrad]
	1995904265776 -> 1995904264576
	1995905105616 [label="conv5.bn2.weight
 (1024)" fillcolor=lightblue]
	1995905105616 -> 1995904265776
	1995904265776 [label=AccumulateGrad]
	1995904266928 -> 1995904264576
	1995905105696 [label="conv5.bn2.bias
 (1024)" fillcolor=lightblue]
	1995905105696 -> 1995904266928
	1995904266928 [label=AccumulateGrad]
	1995904269808 -> 1995904257136
	1995934386528 [label="up4.weight
 (1024, 512, 2, 2)" fillcolor=lightblue]
	1995934386528 -> 1995904269808
	1995904269808 [label=AccumulateGrad]
	1995904254064 -> 1995904257136
	1995934386608 [label="up4.bias
 (512)" fillcolor=lightblue]
	1995934386608 -> 1995904254064
	1995904254064 [label=AccumulateGrad]
	1995904267984 -> 1995904266880
	1995934384448 [label="att4.W_g.0.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1995934384448 -> 1995904267984
	1995904267984 [label=AccumulateGrad]
	1995904262128 -> 1995904266880
	1995934384528 [label="att4.W_g.0.bias
 (256)" fillcolor=lightblue]
	1995934384528 -> 1995904262128
	1995904262128 [label=AccumulateGrad]
	1995904263088 -> 1995904266592
	1995934384608 [label="att4.W_g.1.weight
 (256)" fillcolor=lightblue]
	1995934384608 -> 1995904263088
	1995904263088 [label=AccumulateGrad]
	1995904258096 -> 1995904266592
	1995934384688 [label="att4.W_g.1.bias
 (256)" fillcolor=lightblue]
	1995934384688 -> 1995904258096
	1995904258096 [label=AccumulateGrad]
	1995904269856 -> 1995904265488
	1995904269856 [label=NativeBatchNormBackward0]
	1995904264624 -> 1995904269856
	1995904264624 [label=ConvolutionBackward0]
	1995904254784 -> 1995904264624
	1995904264144 -> 1995904264624
	1995934385168 [label="att4.W_x.0.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1995934385168 -> 1995904264144
	1995904264144 [label=AccumulateGrad]
	1995904262320 -> 1995904264624
	1995934385248 [label="att4.W_x.0.bias
 (256)" fillcolor=lightblue]
	1995934385248 -> 1995904262320
	1995904262320 [label=AccumulateGrad]
	1995904265008 -> 1995904269856
	1995934385328 [label="att4.W_x.1.weight
 (256)" fillcolor=lightblue]
	1995934385328 -> 1995904265008
	1995904265008 [label=AccumulateGrad]
	1995904266784 -> 1995904269856
	1995934385408 [label="att4.W_x.1.bias
 (256)" fillcolor=lightblue]
	1995934385408 -> 1995904266784
	1995904266784 [label=AccumulateGrad]
	1995904265536 -> 1995904260256
	1995934385888 [label="att4.psi.0.weight
 (1, 256, 1, 1)" fillcolor=lightblue]
	1995934385888 -> 1995904265536
	1995904265536 [label=AccumulateGrad]
	1995904267648 -> 1995904260256
	1995934385968 [label="att4.psi.0.bias
 (1)" fillcolor=lightblue]
	1995934385968 -> 1995904267648
	1995904267648 [label=AccumulateGrad]
	1995904270000 -> 1995904258528
	1995934386048 [label="att4.psi.1.weight
 (1)" fillcolor=lightblue]
	1995934386048 -> 1995904270000
	1995904270000 [label=AccumulateGrad]
	1995904261456 -> 1995904258528
	1995934386128 [label="att4.psi.1.bias
 (1)" fillcolor=lightblue]
	1995934386128 -> 1995904261456
	1995904261456 [label=AccumulateGrad]
	1995904257136 -> 1995904257568
	1995904259200 -> 1995904261120
	1995934386768 [label="dec4.conv1.weight
 (512, 1024, 3, 3)" fillcolor=lightblue]
	1995934386768 -> 1995904259200
	1995904259200 [label=AccumulateGrad]
	1995904258768 -> 1995904261120
	1995934386848 [label="dec4.conv1.bias
 (512)" fillcolor=lightblue]
	1995934386848 -> 1995904258768
	1995904258768 [label=AccumulateGrad]
	1995904261648 -> 1995904261360
	1995934386928 [label="dec4.bn1.weight
 (512)" fillcolor=lightblue]
	1995934386928 -> 1995904261648
	1995904261648 [label=AccumulateGrad]
	1995904260784 -> 1995904261360
	1995934387008 [label="dec4.bn1.bias
 (512)" fillcolor=lightblue]
	1995934387008 -> 1995904260784
	1995904260784 [label=AccumulateGrad]
	1995904261024 -> 1995904269136
	1995934387488 [label="dec4.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1995934387488 -> 1995904261024
	1995904261024 [label=AccumulateGrad]
	1995904259728 -> 1995904269136
	1995934387568 [label="dec4.conv2.bias
 (512)" fillcolor=lightblue]
	1995934387568 -> 1995904259728
	1995904259728 [label=AccumulateGrad]
	1995904269232 -> 1995904267168
	1995934387648 [label="dec4.bn2.weight
 (512)" fillcolor=lightblue]
	1995934387648 -> 1995904269232
	1995904269232 [label=AccumulateGrad]
	1995904269520 -> 1995904267168
	1995934387728 [label="dec4.bn2.bias
 (512)" fillcolor=lightblue]
	1995934387728 -> 1995904269520
	1995904269520 [label=AccumulateGrad]
	1995904258336 -> 1995904967024
	1995934390288 [label="up3.weight
 (512, 256, 2, 2)" fillcolor=lightblue]
	1995934390288 -> 1995904258336
	1995904258336 [label=AccumulateGrad]
	1995904270144 -> 1995904967024
	1995934390368 [label="up3.bias
 (256)" fillcolor=lightblue]
	1995934390368 -> 1995904270144
	1995904270144 [label=AccumulateGrad]
	1995904269472 -> 1995904266256
	1995934388128 [label="att3.W_g.0.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1995934388128 -> 1995904269472
	1995904269472 [label=AccumulateGrad]
	1995904270048 -> 1995904266256
	1995934388208 [label="att3.W_g.0.bias
 (128)" fillcolor=lightblue]
	1995934388208 -> 1995904270048
	1995904270048 [label=AccumulateGrad]
	1995904265104 -> 1995904256032
	1995934388288 [label="att3.W_g.1.weight
 (128)" fillcolor=lightblue]
	1995934388288 -> 1995904265104
	1995904265104 [label=AccumulateGrad]
	1995904263664 -> 1995904256032
	1995934388368 [label="att3.W_g.1.bias
 (128)" fillcolor=lightblue]
	1995934388368 -> 1995904263664
	1995904263664 [label=AccumulateGrad]
	1995904258048 -> 1995904255792
	1995904258048 [label=NativeBatchNormBackward0]
	1995904265392 -> 1995904258048
	1995904265392 [label=ConvolutionBackward0]
	1992578027856 -> 1995904265392
	1995904254112 -> 1995904265392
	1995934388848 [label="att3.W_x.0.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1995934388848 -> 1995904254112
	1995904254112 [label=AccumulateGrad]
	1995904260880 -> 1995904265392
	1995934388928 [label="att3.W_x.0.bias
 (128)" fillcolor=lightblue]
	1995934388928 -> 1995904260880
	1995904260880 [label=AccumulateGrad]
	1995904258192 -> 1995904258048
	1995934389008 [label="att3.W_x.1.weight
 (128)" fillcolor=lightblue]
	1995934389008 -> 1995904258192
	1995904258192 [label=AccumulateGrad]
	1995904267216 -> 1995904258048
	1995934389088 [label="att3.W_x.1.bias
 (128)" fillcolor=lightblue]
	1995934389088 -> 1995904267216
	1995904267216 [label=AccumulateGrad]
	1995904255456 -> 1992525308128
	1995934389568 [label="att3.psi.0.weight
 (1, 128, 1, 1)" fillcolor=lightblue]
	1995934389568 -> 1995904255456
	1995904255456 [label=AccumulateGrad]
	1995904263952 -> 1992525308128
	1995934389648 [label="att3.psi.0.bias
 (1)" fillcolor=lightblue]
	1995934389648 -> 1995904263952
	1995904263952 [label=AccumulateGrad]
	1991803681568 -> 1992598012208
	1995934389728 [label="att3.psi.1.weight
 (1)" fillcolor=lightblue]
	1995934389728 -> 1991803681568
	1991803681568 [label=AccumulateGrad]
	1995904266208 -> 1992598012208
	1995934389808 [label="att3.psi.1.bias
 (1)" fillcolor=lightblue]
	1995934389808 -> 1995904266208
	1995904266208 [label=AccumulateGrad]
	1995904967024 -> 1995904967984
	1995904973840 -> 1995904967792
	1995934390528 [label="dec3.conv1.weight
 (256, 512, 3, 3)" fillcolor=lightblue]
	1995934390528 -> 1995904973840
	1995904973840 [label=AccumulateGrad]
	1995904968368 -> 1995904967792
	1995934390608 [label="dec3.conv1.bias
 (256)" fillcolor=lightblue]
	1995934390608 -> 1995904968368
	1995904968368 [label=AccumulateGrad]
	1995904967840 -> 1995904970096
	1995934390688 [label="dec3.bn1.weight
 (256)" fillcolor=lightblue]
	1995934390688 -> 1995904967840
	1995904967840 [label=AccumulateGrad]
	1995904972544 -> 1995904970096
	1995934390768 [label="dec3.bn1.bias
 (256)" fillcolor=lightblue]
	1995934390768 -> 1995904972544
	1995904972544 [label=AccumulateGrad]
	1995904971008 -> 1995904967120
	1995934391248 [label="dec3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1995934391248 -> 1995904971008
	1995904971008 [label=AccumulateGrad]
	1995904970336 -> 1995904967120
	1995934391328 [label="dec3.conv2.bias
 (256)" fillcolor=lightblue]
	1995934391328 -> 1995904970336
	1995904970336 [label=AccumulateGrad]
	1995904973120 -> 1995904968992
	1995934391408 [label="dec3.bn2.weight
 (256)" fillcolor=lightblue]
	1995934391408 -> 1995904973120
	1995904973120 [label=AccumulateGrad]
	1995904968896 -> 1995904968992
	1995934391488 [label="dec3.bn2.bias
 (256)" fillcolor=lightblue]
	1995934391488 -> 1995904968896
	1995904968896 [label=AccumulateGrad]
	1995904972832 -> 1995904970288
	1995934394128 [label="up2.weight
 (256, 128, 2, 2)" fillcolor=lightblue]
	1995934394128 -> 1995904972832
	1995904972832 [label=AccumulateGrad]
	1995904966784 -> 1995904970288
	1995934394208 [label="up2.bias
 (128)" fillcolor=lightblue]
	1995934394208 -> 1995904966784
	1995904966784 [label=AccumulateGrad]
	1995904973936 -> 1995904972448
	1995934391968 [label="att2.W_g.0.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	1995934391968 -> 1995904973936
	1995904973936 [label=AccumulateGrad]
	1995904973696 -> 1995904972448
	1995934392048 [label="att2.W_g.0.bias
 (64)" fillcolor=lightblue]
	1995934392048 -> 1995904973696
	1995904973696 [label=AccumulateGrad]
	1995904971152 -> 1995904974080
	1995934392128 [label="att2.W_g.1.weight
 (64)" fillcolor=lightblue]
	1995934392128 -> 1995904971152
	1995904971152 [label=AccumulateGrad]
	1995904972400 -> 1995904974080
	1995934392208 [label="att2.W_g.1.bias
 (64)" fillcolor=lightblue]
	1995934392208 -> 1995904972400
	1995904972400 [label=AccumulateGrad]
	1995904973792 -> 1995904972352
	1995904973792 [label=NativeBatchNormBackward0]
	1995904968272 -> 1995904973792
	1995904968272 [label=ConvolutionBackward0]
	1995904971584 -> 1995904968272
	1995904970432 -> 1995904968272
	1995934392688 [label="att2.W_x.0.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	1995934392688 -> 1995904970432
	1995904970432 [label=AccumulateGrad]
	1995904971104 -> 1995904968272
	1995934392768 [label="att2.W_x.0.bias
 (64)" fillcolor=lightblue]
	1995934392768 -> 1995904971104
	1995904971104 [label=AccumulateGrad]
	1995904974608 -> 1995904973792
	1995934392848 [label="att2.W_x.1.weight
 (64)" fillcolor=lightblue]
	1995934392848 -> 1995904974608
	1995904974608 [label=AccumulateGrad]
	1995904972304 -> 1995904973792
	1995934392928 [label="att2.W_x.1.bias
 (64)" fillcolor=lightblue]
	1995934392928 -> 1995904972304
	1995904972304 [label=AccumulateGrad]
	1995904974368 -> 1995904973216
	1995934393408 [label="att2.psi.0.weight
 (1, 64, 1, 1)" fillcolor=lightblue]
	1995934393408 -> 1995904974368
	1995904974368 [label=AccumulateGrad]
	1995904972640 -> 1995904973216
	1995934393488 [label="att2.psi.0.bias
 (1)" fillcolor=lightblue]
	1995934393488 -> 1995904972640
	1995904972640 [label=AccumulateGrad]
	1995904973312 -> 1995904973360
	1995934393568 [label="att2.psi.1.weight
 (1)" fillcolor=lightblue]
	1995934393568 -> 1995904973312
	1995904973312 [label=AccumulateGrad]
	1995904970048 -> 1995904973360
	1995934393648 [label="att2.psi.1.bias
 (1)" fillcolor=lightblue]
	1995934393648 -> 1995904970048
	1995904970048 [label=AccumulateGrad]
	1995904970288 -> 1995904970576
	1995904971920 -> 1995904972256
	1995934394368 [label="dec2.conv1.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	1995934394368 -> 1995904971920
	1995904971920 [label=AccumulateGrad]
	1995904970528 -> 1995904972256
	1995934394448 [label="dec2.conv1.bias
 (128)" fillcolor=lightblue]
	1995934394448 -> 1995904970528
	1995904970528 [label=AccumulateGrad]
	1995904972160 -> 1995904970480
	1995934394528 [label="dec2.bn1.weight
 (128)" fillcolor=lightblue]
	1995934394528 -> 1995904972160
	1995904972160 [label=AccumulateGrad]
	1995904970864 -> 1995904970480
	1995934394608 [label="dec2.bn1.bias
 (128)" fillcolor=lightblue]
	1995934394608 -> 1995904970864
	1995904970864 [label=AccumulateGrad]
	1995904970960 -> 1995904966496
	1995934395088 [label="dec2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1995934395088 -> 1995904970960
	1995904970960 [label=AccumulateGrad]
	1995904970912 -> 1995904966496
	1995934395168 [label="dec2.conv2.bias
 (128)" fillcolor=lightblue]
	1995934395168 -> 1995904970912
	1995904970912 [label=AccumulateGrad]
	1995904967600 -> 1995904967696
	1995934395248 [label="dec2.bn2.weight
 (128)" fillcolor=lightblue]
	1995934395248 -> 1995904967600
	1995904967600 [label=AccumulateGrad]
	1995904968848 -> 1995904967696
	1995934395328 [label="dec2.bn2.bias
 (128)" fillcolor=lightblue]
	1995934395328 -> 1995904968848
	1995904968848 [label=AccumulateGrad]
	1995904969184 -> 1995904964912
	1995934397888 [label="up1.weight
 (128, 64, 2, 2)" fillcolor=lightblue]
	1995934397888 -> 1995904969184
	1995904969184 [label=AccumulateGrad]
	1995904967936 -> 1995904964912
	1995934397968 [label="up1.bias
 (64)" fillcolor=lightblue]
	1995934397968 -> 1995904967936
	1995904967936 [label=AccumulateGrad]
	1995904969424 -> 1995904968224
	1995934395808 [label="att1.W_g.0.weight
 (32, 64, 1, 1)" fillcolor=lightblue]
	1995934395808 -> 1995904969424
	1995904969424 [label=AccumulateGrad]
	1995904967888 -> 1995904968224
	1995934395888 [label="att1.W_g.0.bias
 (32)" fillcolor=lightblue]
	1995934395888 -> 1995904967888
	1995904967888 [label=AccumulateGrad]
	1995904966928 -> 1995937004944
	1995934395968 [label="att1.W_g.1.weight
 (32)" fillcolor=lightblue]
	1995934395968 -> 1995904966928
	1995904966928 [label=AccumulateGrad]
	1995904968176 -> 1995937004944
	1995934396048 [label="att1.W_g.1.bias
 (32)" fillcolor=lightblue]
	1995934396048 -> 1995904968176
	1995904968176 [label=AccumulateGrad]
	1995914700176 -> 1995904969856
	1995914700176 [label=NativeBatchNormBackward0]
	1992598020560 -> 1995914700176
	1992598020560 [label=ConvolutionBackward0]
	1995909375376 -> 1992598020560
	1995904969280 -> 1992598020560
	1995934396528 [label="att1.W_x.0.weight
 (32, 64, 1, 1)" fillcolor=lightblue]
	1995934396528 -> 1995904969280
	1995904969280 [label=AccumulateGrad]
	1995904970816 -> 1992598020560
	1995934396608 [label="att1.W_x.0.bias
 (32)" fillcolor=lightblue]
	1995934396608 -> 1995904970816
	1995904970816 [label=AccumulateGrad]
	1995904969520 -> 1995914700176
	1995934396688 [label="att1.W_x.1.weight
 (32)" fillcolor=lightblue]
	1995934396688 -> 1995904969520
	1995904969520 [label=AccumulateGrad]
	1995904969616 -> 1995914700176
	1995934396768 [label="att1.W_x.1.bias
 (32)" fillcolor=lightblue]
	1995934396768 -> 1995904969616
	1995904969616 [label=AccumulateGrad]
	1995904968128 -> 1995904964816
	1995934397248 [label="att1.psi.0.weight
 (1, 32, 1, 1)" fillcolor=lightblue]
	1995934397248 -> 1995904968128
	1995904968128 [label=AccumulateGrad]
	1995904964528 -> 1995904964816
	1995934397328 [label="att1.psi.0.bias
 (1)" fillcolor=lightblue]
	1995934397328 -> 1995904964528
	1995904964528 [label=AccumulateGrad]
	1995904961600 -> 1995936204784
	1995934397408 [label="att1.psi.1.weight
 (1)" fillcolor=lightblue]
	1995934397408 -> 1995904961600
	1995904961600 [label=AccumulateGrad]
	1995904961888 -> 1995936204784
	1995934397488 [label="att1.psi.1.bias
 (1)" fillcolor=lightblue]
	1995934397488 -> 1995904961888
	1995904961888 [label=AccumulateGrad]
	1995904964912 -> 1995904960016
	1995904959968 -> 1995904958576
	1995934398128 [label="dec1.conv1.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	1995934398128 -> 1995904959968
	1995904959968 [label=AccumulateGrad]
	1995904958960 -> 1995904958576
	1995934398208 [label="dec1.conv1.bias
 (64)" fillcolor=lightblue]
	1995934398208 -> 1995904958960
	1995904958960 [label=AccumulateGrad]
	1995904958528 -> 1995904958864
	1995934398288 [label="dec1.bn1.weight
 (64)" fillcolor=lightblue]
	1995934398288 -> 1995904958528
	1995904958528 [label=AccumulateGrad]
	1995904960112 -> 1995904958864
	1995934398368 [label="dec1.bn1.bias
 (64)" fillcolor=lightblue]
	1995934398368 -> 1995904960112
	1995904960112 [label=AccumulateGrad]
	1995909286256 -> 1995909109632
	1995934398848 [label="dec1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1995934398848 -> 1995909286256
	1995909286256 [label=AccumulateGrad]
	1995909299744 -> 1995909109632
	1995934398928 [label="dec1.conv2.bias
 (64)" fillcolor=lightblue]
	1995934398928 -> 1995909299744
	1995909299744 [label=AccumulateGrad]
	1995908854736 -> 1995936208336
	1995934399008 [label="dec1.bn2.weight
 (64)" fillcolor=lightblue]
	1995934399008 -> 1995908854736
	1995908854736 [label=AccumulateGrad]
	1995914594672 -> 1995936208336
	1995934399088 [label="dec1.bn2.bias
 (64)" fillcolor=lightblue]
	1995934399088 -> 1995914594672
	1995914594672 [label=AccumulateGrad]
	1995936172448 -> 1995936289824
	1995934399568 [label="final.weight
 (1, 64, 1, 1)" fillcolor=lightblue]
	1995934399568 -> 1995936172448
	1995936172448 [label=AccumulateGrad]
	1995908992592 -> 1995936289824
	1995934399648 [label="final.bias
 (1)" fillcolor=lightblue]
	1995934399648 -> 1995908992592
	1995908992592 [label=AccumulateGrad]
	1995905452784 -> 1992601245792
}
